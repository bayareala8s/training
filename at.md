The proposed solution delivers several key benefits aligned with enterprise architecture principles. From a resiliency perspective, the system is designed with multi-region high availability and disaster recovery (HA/DR), enabling controlled failover behavior across regions to ensure continuity of operations. Security is implemented following Zero Trust principles, incorporating strong identity enforcement, encryption in transit and at rest, and least-privilege access controls to protect sensitive data and credentials. Reliability is achieved through idempotent execution mechanisms and distributed locking strategies that prevent duplicate file delivery and ensure consistent outcomes even under retry scenarios. The architecture is highly scalable, supporting file sizes ranging from small payloads to very large transfers with predictable performance characteristics. Operational excellence is supported through centralized logging, metrics, monitoring, documented runbooks, and defined rollback strategies to enable effective incident response and maintainability. Finally, while the solution is backend-focused, it provides self-service capabilities through APIs that allow customers to onboard SFTP and S3 endpoints and track transfer status programmatically, enabling transparency and automation without requiring a graphical interface.

The architecture is built on several key design decisions intended to balance resiliency, scalability, and operational control. First, the system adopts a partitioned active-active execution model across regions. While both regions are capable of accepting requests, execution for a given workload partition is controlled through a distributed locking mechanism implemented using Amazon DynamoDB. This ensures that only one region actively processes a specific partition at a time, preventing duplicate transfers while still enabling seamless failover and high availability.

Second, the transfer execution layer leverages AWS Fargate-based workers for SFTP operations. This approach supports streaming large files efficiently, handles retries in a controlled manner, and allows fine-grained control over network egress. By using containerized workers, the system can scale dynamically based on workload demand while maintaining isolation and predictable performance characteristics for file sizes ranging from small payloads to large multi-gigabyte transfers.

Third, Amazon S3 is used as a staging layer for SFTP-to-SFTP transfers. Introducing S3 as an intermediary storage layer improves reliability and disaster recovery by decoupling source and destination systems. It also enables operational re-drive capabilities, allowing failed transfers to be retried without requiring re-ingestion from the original source endpoint. This staging pattern enhances durability and provides greater observability into transfer state.

Fourth, the architecture follows an event-driven model. File arrival events in S3, along with scheduled triggers, initiate workflows through Amazon EventBridge and Amazon SQS, which in turn invoke AWS Step Functions for orchestration. This decoupled design promotes scalability, fault tolerance, and clear separation between event ingestion, workflow coordination, and execution.

Finally, endpoint configuration is managed programmatically through APIs. Customers register SFTP and S3 endpoints via a POST-based API interface, with credentials securely stored in AWS Secrets Manager and configuration metadata persisted in DynamoDB. This design ensures secure secret handling, consistent configuration management, and auditability, while enabling automation without requiring a graphical interface.



